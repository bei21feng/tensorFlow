{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 神经网络简史\n",
    "Neural Network 30年前流行过\n",
    "1980-2000 转而流行支持向量机(Support Vector Machine, SVM)\n",
    "网络层不多(1-3) 浅层神经网络(Shallow Neural Network)\n",
    "深层神经网络(Deep Neural Network) 得益于：\n",
    "- 运算能力 使用GPU,TPU\n",
    "- 数据数量\n",
    "- 激活函数 以前使用Sigmoid,网络层数太深导致梯度消失(Vanishing Gradient) ReLU函数可以有效避免"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 神经网络原理\n",
    "- Input Layer, Hidden Layer, Output Layer\n",
    "- 每层有很多Neuron，各层之间连接依赖 权重(weight)，偏差(bias)\n",
    "- 每层最后输出需要激活函数(否则无论网络多深，学习到的只是一个线性方程)\n",
    "  - 常见激活函数ReLU, Sigmoid, Tanh\n",
    "\n",
    "全连接(Fully Connected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 损失函数 MSE 和 MAE\n",
    "  Lost Function 用来计算模型预测值和预期输出值之间的相似程度\n",
    "  对于机器学习不同问题的常用损失函数\n",
    "  - 回归问题：均方差(Mean Squared Error, MSE)，平均绝对误差(Mean Absolute Error, MAE)\n",
    "  - 二分类问题：二分类交叉熵(Binary Cross-Entropy, BCE)\n",
    "  - 多分类问题：多分类交叉熵(Categorical Cross-Entropy, CCE)\n",
    "\n",
    "预测结果在1，-1之间 ： MAE损失较大\n",
    "预测结果在之外 ： MSE损失较大"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 神经网络权重更新\n",
    "- 梯度下降(Gradient Descent, GD)\n",
    "\n",
    "W：权重 横坐标\n",
    "L: 损失函数 纵坐标\n",
    "eta: 学习率\n",
    "w = w-eta*导数\n",
    "\n",
    "可能并非全局最小(Global Minimum),而是局部最小(Local Minimum)\n",
    "\n",
    "- 学习率(Learning Rate)\n",
    "\n",
    "过大：错过最小损失值\n",
    "过小：效率过低\n",
    "\n",
    "- 梯度更新方法\n",
    "\n",
    "传统：一次使用全部的训练数据计算损失函数的梯度，并更新一次权重。（更新N次，要计算整个训练数据N遍）\n",
    "\n",
    "__随机梯度下降法__(Stochastic Gradient Descent, SGD):\n",
    "  一次计算一个批量(Batch)数据的梯度值，并更新一次权重\n",
    "\n",
    "其他方法：Momentum(加入动量概念), AdaGrad(根据梯度调整学习率), Adam(结合，效果不错)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 神经网络训练步骤\n",
    "- 准备训练数据(Training Data)  分为训练数据(Input X),预期输出值的标记答案(Output Y)\n",
    "- 搭建神经网络模型(Model)  用以预测数值(Prediction, y帽)\n",
    "- 损失函数(Loss Function)  计算模型预测值与标记答案的误差\n",
    "- 优化器(Optimizer)  决定学习过程的进行，常用方法：SGD, Adam,RMSprop\n",
    "- 准备验证数据(Validation Data)和测试数据(Testing Data)  让网络模型进行预测，通过评价指标函数(Metrics Function)来评价模型好坏\n",
    "    - 验证数据：训练过程的超参数(网络层数，神经元个数)，调整过程利用验证数据的性能\n",
    "    - 测试数据:比赛中保留的，最后测评(真实世界指标)，否则可能元过拟合(Meta-Overfitting)\n",
    "\n",
    "注：模型好坏程度：评价指标函数/损失值。可能相同，也可不同\n",
    "    超参数(Hyperparameter)训练网络模型之前设置的参数（批量，学习率，训练周期，网络层数，网络节点数）"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}